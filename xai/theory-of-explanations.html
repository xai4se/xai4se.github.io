
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>A Theory of Explanations &#8212; Explainable AI for SE</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://xai4se.github.io/book/xai/theory-of-explanations.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Techniques for Generating Explanations" href="techniques-for-generating-explanations.html" />
    <link rel="prev" title="Explainable AI for Software Engineering" href="../index.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://xai4se.github.io/book/xai/theory-of-explanations.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="A Theory of Explanations" />
<meta property="og:description" content="A Theory of Explanations  A Theory of Explainability  According to philosophy, social science, and psychology theories, a common definition of explainability or" />
<meta property="og:image"       content="https://xai4se.github.io/book/_static/logo.png" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Explainable AI for SE</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Explainable AI for Software Engineering
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Part1-What is Explainable AI?
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   A Theory of Explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="techniques-for-generating-explanations.html">
   Techniques for Generating Explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-specific-techniques.html">
   Model-specific Techniques for Generating Global Explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-agnostic-techniques.html">
   Model-agnostic Techniques for Generating Local Explanations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Part2-Defect Prediction Models
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/software-quality-assurance.html">
   Software Quality Assurance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/defect-prediction.html">
   Defect Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/data-collection.html">
   (Step 1) Data Collection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/data-preprocessing.html">
   (Step 2) Data Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/model-construction.html">
   (Step 3) Model Construction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/model-evaluation.html">
   (Step 4) Model Evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../defect-prediction/model-ranking.html">
   (Step 5) Model Ranking
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Part3-Explainable AI for Software Engineering
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/explainability-for-se.html">
   Explainability in Software Engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/a-case-study-of-defect-prediction.html">
   A Case Study of Defect Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/defective-line-localization.html">
   (Example 1) Help developers localize which lines of code are the most risky
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/local-defect-explanation.html">
   (Example 2) Help developers understand why a file is predicted as defective
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../xai4se/specific-sq-plan.html">
   (Example 3) Help managers develop software quality improvement plans
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/xai/theory-of-explanations.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xai4se/book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xai4se/book/issues/new?title=Issue%20on%20page%20%2Fxai/theory-of-explanations.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/xai4se/book/edit/master/docs/xai/theory-of-explanations.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/xai4se/book/master?urlpath=lab/tree/docs/xai/theory-of-explanations.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/xai4se/book/blob/master/docs/xai/theory-of-explanations.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-theory-of-explainability">
   A Theory of Explainability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#explainability-goals">
   Explainability Goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-explanations">
   What are Explanations?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-intelligibility-questions">
   Types of Intelligibility Questions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scopes-of-explanations">
   Scopes of Explanations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-explanations">
   Types of Explanations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#suggested-readings">
   Suggested Readings
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="a-theory-of-explanations">
<h1>A Theory of Explanations<a class="headerlink" href="#a-theory-of-explanations" title="Permalink to this headline">¶</a></h1>
<div class="section" id="a-theory-of-explainability">
<h2>A Theory of Explainability<a class="headerlink" href="#a-theory-of-explainability" title="Permalink to this headline">¶</a></h2>
<p>According to philosophy, social science, and psychology theories, a
common definition of <em>explainability or interpretability</em> is <em>the degree
to which a human can understand the reasons behind a decision or an
action</em> <span id="id1">[<a class="reference internal" href="../References.html#id213"><span>Mil19</span></a>]</span>. The explainability of AI/ML algorithms can be
achieved by (1) making the entire decision-making process transparent
and comprehensible and (2) explicitly providing an explanation for each
decision <span id="id2">[<a class="reference internal" href="../References.html#id172"><span>Lip18</span></a>]</span> (since an explanation is not likely applicable to
all decisions <span id="id3">[<a class="reference internal" href="../References.html#id164"><span>Lea14</span></a>]</span>. Hence, research has emerged to
explore how to explain decisions made by complex, black-box models and
how explanations are presented in a form that would be easily understood
(and hence, accepted) by humans.</p>
</div>
<div class="section" id="explainability-goals">
<h2>Explainability Goals<a class="headerlink" href="#explainability-goals" title="Permalink to this headline">¶</a></h2>
<p>Explanations of AI/ML systems can be presented in various forms to serve various goals of different stakeholders.
For example, the goal of software managers is to understand the general characteristics of software projects that are associated with defect-proneness to chart appropriate qualitity improvement plans.
Thus, explanations needed by such software managers are generic explanations that describe the whole reasons and logic behind decisions of AI/ML systems.
On the other hand, the goal of software developers is to understand why a particular file is predicted as defective by AI/ML systems to mitigate the risk of such file being defective.
In this case, explanations needed by software developers are explanations that are specific to such file and describe the reasons and logic behind defective predictions made by AI/ML systems.</p>
<!-- 
TODO?
Different stakeholders have different goals, thus require different forms of explanations. -->
</div>
<div class="section" id="what-are-explanations">
<h2>What are Explanations?<a class="headerlink" href="#what-are-explanations" title="Permalink to this headline">¶</a></h2>
<p>According to a philosophical and psychological theory of explanations,
Salmon  <span id="id4">[<a class="reference internal" href="../References.html#id268"><span>Sal84</span></a>]</span> argue that explanations can be presented as a causal
chain of causes that lead to the decision. Causal chains can be
classified into five categories <span id="id5">[<a class="reference internal" href="../References.html#id107"><span>HMS05</span></a>]</span>: temporal, coincidental,
unfolding, opportunity chains and pre-emptive. Each type of causal chain
is thus associated with an explanation type. However, identifying the
complete causal chain of causes is challenging, since most AI/ML
techniques produce only correlations instead of causations.
In contrast, Miller <span id="id6">[<a class="reference internal" href="../References.html#id213"><span>Mil19</span></a>]</span> argue that explanations can be presented
as answers to why-questions. Similarly, Lipton <span id="id7">[<a class="reference internal" href="../References.html#id718"><span>Lip90</span></a>]</span> also share
a similar view of explanations as being contrastive.</p>
</div>
<div class="section" id="types-of-intelligibility-questions">
<h2>Types of Intelligibility Questions<a class="headerlink" href="#types-of-intelligibility-questions" title="Permalink to this headline">¶</a></h2>
<p>Lim et al. <span id="id8">[<a class="reference internal" href="../References.html#id170"><span>LYAW19</span></a>]</span> categorized questions towards the inference mechanism of AI/ML systems into: What, Why, Why Not, What If, and How To.
Below, we provide an example of questions for each type of the intelligibility questions.</p>
<ul class="simple">
<li><p><em>What</em>: What is the logic behind the AI/ML models?</p></li>
<li><p><em>Why</em>: Why is an instance predicted as TRUE?</p></li>
<li><p><em>Why Not</em>: Why is an instance predicted as TRUE?</p></li>
<li><p><em>How To</em>: How can we reverse the prediction of an instance (e.g., from TRUE to FALSE) generated by the system?</p></li>
<li><p><em>What If</em>: What would the system predict if the values of an instance are changed?</p></li>
</ul>
<!-- While they initially found that Why and Why Not explanations were most effective in promoting system understanding and trust [65], -->
<!-- they later found that users use different strategies to check model behavior and thus use different intelligibility queries for the same interpretability goals [68, 70].  -->
</div>
<div class="section" id="scopes-of-explanations">
<h2>Scopes of Explanations<a class="headerlink" href="#scopes-of-explanations" title="Permalink to this headline">¶</a></h2>
<p>Explainability can be achieved at two levels:</p>
<p>The explainability of software analytics can be achieved by:</p>
<p><strong>Global Explanability:</strong> Using interpretable machine learning
techniques (e.g., decision tree, decision rules or logistic
regression techniques) or intrinsic model-specific techniques (e.g.,
ANOVA, variable importance) so the entire predictions and
recommendations process are transparent and comprehensible. Such
intrinsic model-specific techniques aim to provide the global
explainability. Thus, users can only understand how the model works
globally (e.g., by inspecting a branch of decision trees). However,
users often do not understand why the model makes that prediction.</p>
<p><strong>Local Explanability:</strong> Using model-agnostic techniques (e.g.,
LIME <span id="id9">[<a class="reference internal" href="../References.html#id257"><span>RSG16b</span></a>]</span>) to explain the predictions of the
software analytics models (e.g., neural network, random forest).
Such post-hoc model-agnostic techniques can provide an explanation
for each prediction (i.e., an instance to be explained). Users can
then understand why the prediction is made by the software analytics
models.</p>
</div>
<div class="section" id="types-of-explanations">
<h2>Types of Explanations<a class="headerlink" href="#types-of-explanations" title="Permalink to this headline">¶</a></h2>
<p>To better explain types of explanations, we first provide definitions of <em>Fact</em> and <em>Foil</em> with examples in the context of defect prediction.</p>
<p><strong>Fact</strong> refers to a cause that is actually happened (what happened).<br />
Example: <em>A.java</em> is predicted as defective.</p>
<p><strong>Foil</strong> refers to an expected or plausible-to-happen cause that is an alternative to Fact.<br />
Examples: (1) <em>A.java</em> is predicted as clean. and (2) <em>B.java</em> is predicted as clean.</p>
<p>With respect to the fact and foil, Lim et al. <span id="id10">[<a class="reference internal" href="../References.html#id170"><span>LYAW19</span></a>]</span> discussed 4 types of explanations as follows:</p>
<ul class="simple">
<li><p><strong>Causal Explanation</strong> refers to an explanation that is focused on selected causes relevant to interpreting the observation with respect to existing knowledge. An example of causal explanation presented as an answer to a question with respect to a fact cause example is provided below.<br />
Example: [Why] Why <em>A.java</em> is predicted as defective?</p></li>
<li><p><strong>Contrastive Explanation</strong> refers to an explanation that is focused on the contrastive between a fact and a foil.
Recently, Van Bouwel and Weber <span id="id11">[<a class="reference internal" href="../References.html#id717"><span>VBW02</span></a>]</span> futher distinguised contrastive explanations into 3 types as follows:</p>
<ul>
<li><p><em>Property-contrast</em> (P-contrast) is the differences in the Properties within an object.<br />
Example: [Why not] Why <em>A.java</em> is predicted as defective, rather than clean?</p></li>
<li><p><em>Object-contrast</em> (O-contrast) is the differences between two Objects.<br />
Example: [Why not] Why <em>A.java</em> is predicted as defective, while <em>B.java</em> is predicted as clean?</p></li>
<li><p><em>Time-contrast</em> is the differences within an object over Time.<br />
Example: [Why not] Why <em>A.java</em> is predicted as defective in the current release, but was predicted as clean in the previous release?</p></li>
</ul>
</li>
<li><p><strong>Counterfactual Explanation</strong> refers to an explanation of what needs to change for an alternative outcome to happen (foil).<br />
Example: [How to] How can we reverse the prediction of <em>A.java</em> generated by the model (from defective to clean)?</p></li>
<li><p><strong>Transfactual Explanation</strong> refers to an explanation that explains a hyperthetical scenario of <em>what if</em> the factors were different, then what the effect would be.
Example: [What if] What would be the prediction of <em>A.java</em> if its size and complexity were changed?</p></li>
</ul>
<!-- 
https://explainablesystems.comp.nus.edu.sg/2019/wp-content/uploads/2019/02/IUI19WS-ExSS2019-20.pdf
https://arxiv.org/pdf/1606.03490.pdf
https://arxiv.org/pdf/2001.02478.pdf
http://www.cs.cmu.edu/~byl/publications/lim_chi09.pdf
https://dl-acm-org.ezproxy.lib.monash.edu.au/doi/pdf/10.1145/3290605.3300831
 -->
<p>Finally, we present a summary table of the scope of explanations, types of explanation, definition, examples, and techniques for generating answers towards such questions with respect to the five types of questions.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Questions</p></th>
<th class="head"><p>Scope of Explanations</p></th>
<th class="head"><p>Types of Explanation</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Examples</p></th>
<th class="head"><p>Techniques</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>What</p></td>
<td><p>Global</p></td>
<td><p>Plain-Fact</p></td>
<td><p>Describe the weights of features used by the model</p></td>
<td><p>What is the logic behind the AI/ML models?</p></td>
<td><p>ANOVA, VarImp, Decision Tree, and Decision Rules</p></td>
</tr>
<tr class="row-odd"><td><p>Why</p></td>
<td><p>Local</p></td>
<td><p>Causal</p></td>
<td><p>Show how features of the instance contribute to the model’s prediction</p></td>
<td><p>Why is an instance predicted as TRUE?</p></td>
<td><p>Supporting Scores of LIME and SHAP</p></td>
</tr>
<tr class="row-even"><td><p>Why not</p></td>
<td><p>Local</p></td>
<td><p>Contrastive</p></td>
<td><p>Contradicting scores?? (not sure how to explain)</p></td>
<td><p>Why is an instance not predicted as FALSE?</p></td>
<td><p>Contradicting Scores of LIME and SHAP</p></td>
</tr>
<tr class="row-odd"><td><p>How To</p></td>
<td><p>Local</p></td>
<td><p>Counterfactual</p></td>
<td><p>Describe what are the features that can reverse the prediction.</p></td>
<td><p>How can we reverse the prediction of an instance generated by the system?</p></td>
<td><p>LORE</p></td>
</tr>
<tr class="row-even"><td><p>What If</p></td>
<td><p>Local</p></td>
<td><p>Transfactual</p></td>
<td><p>Show how the prediction changes corresponding to changes of a feature</p></td>
<td><p>What would the system predict if the values of an instance are changed?</p></td>
<td><p>PyExplainer</p></td>
</tr>
</tbody>
</table>
<!-- 

five types of questions as well as their examples and the techniques used to generate explanations towards them:

- **What**

While a reasoning trace typically addresses the question of
why and how the system did something, there are
several other questions that end-users of novel systems may
ask. We chose to look into the following questions (adapted
from [11]):
1. What: What did the system do?
2. Why: Why did the system do W?
3. Why Not: Why did the system not do X?
4. What If: What would the system do if Y happens?
5. How To: How can I get the system to do Z, given the
current context?

 -->
<!-- Answers to the P-contrast, O-contrast and T-contrast why-questions form
an explanation. *Contrastive explanations focus on only the differences
on **Properties within an object** (Property-contrast), between **two
Objects** (Object-contrast), and **within an object over Time**
(Time-contrast)* {cite}`VanBouwel2002`. Answering a plain fact question is
generally more difficult than generating answers to the contrastive
questions {cite}`lipton1990contrastive`. For example, we could answer the
Property-contrast question (e.g., "Why is file $A$ classified as being
defective instead of being clean?") by citing that there are a
substantial number of defect-fixing commits that involve with the file.
Information about the size, complexity, owner of the file, and so on are
not required to answer this question. On the other hand, explaining why
file $A$ is defective in a non-contrastive manner would require us to
use all causes. In addition, humans tend to be cognitively attached to
digest contrastive explanations {cite}`Miller19`. Thus, contrastive
explanations may be more valuable and more intuitive to humans. These
important factors from both social and computational perspectives should
be considered when providing explainable models or tool support for
software engineering.

Explanation is not only a *product*, as discussed above, but also a
*process* {cite}`Lombrozo2006`. In fact, generating explanations is a
*cognitive process* which essentially involves four cognitive systems:
(1) attention, (2) long-term memory, (3) working memory, and (4)
metacognition {cite}`Horne2019`{cite}`leake2014evaluating`. Recent
work {cite}`Miller19` further recognised the importance of considering
explanation as being not only a cognitive process but also a *social
process*, in which an explainer communicates knowledge to an explainee.
Using this view, explanations should be considered as part of a
conversation between the explainer and explainee. The theories, models,
and processes of how humans explain decisions to one another are
important to the work on explainable software analytics and the
development of explainable tool support for software engineering in
general. -->
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parts of this chapter have been published by Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Hoa Khanh Dam, John Grundy: An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models. IEEE Trans. Software Eng. (2020) <a class="reference external" href="https://doi.org/10.1109/TSE.2020.2982385">https://doi.org/10.1109/TSE.2020.2982385</a>”</p>
</div>
</div>
<div class="section" id="suggested-readings">
<h2>Suggested Readings<a class="headerlink" href="#suggested-readings" title="Permalink to this headline">¶</a></h2>
<p>[1] Brian Y. Lim, Qian Yang, Ashraf M. Abdul, Danding Wang:
Why these Explanations? Selecting Intelligibility Types for Explanation Goals. IUI Workshops 2019.</p>
<p>[2] David K. Lewis: Causal Explanation. Philosophical Papers 2: 214-240 (1986).</p>
<p>[3] Eric Barnes: Why P Rather than Q?: The Curiosities of Fact and Foil. Philosophical Studies 73: 35-53 (1995).</p>
<p>[4] Peter Lipton: Contrastive Explanation. Royal Institute of Philosophy Supplements 27:247-266 (1990).</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "xaitools"
        },
        kernelOptions: {
            kernelName: "xaitools",
            path: "./xai"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'xaitools'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../index.html" title="previous page"><em>Explainable AI for Software Engineering</em></a>
    <a class='right-next' id="next-link" href="techniques-for-generating-explanations.html" title="next page">Techniques for Generating Explanations</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Chakkrit Tantithamthavorn and Jirayus Jiarpakdee<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <script>mermaid.init();</script> <div class="row_footer"> This project has received funding from the <a href="https://www.arc.gov.au/">Australian Research Council</a>'s Discovery Early Career Researcher Award (ARC DECRA) funding scheme (DE200100941). This book reflects the views of the authors and neither Australian Research Council nor Monash University are liable for any use that may be made of the information contained herein. The content of this project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The source code that is part of the content, as well as the source code used to format and display that content is licensed under the MIT License. </div>
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-54962993-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>