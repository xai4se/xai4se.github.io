{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Step 4) Model Evaluation\n",
    "<!-- # - file: defect-prediction/model-validation.ipynb\n",
    "# - file: defect-prediction/evaluation-measures.ipynb -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Always select evaluation measures according to the goals of studies \n",
    "\n",
    "Different studies often have different goals of developing defect models.\n",
    "For example, one study may want its defect models that yield the highest discriminatory power (i.e., optimise based on Area Under the receiver operator characteristic Curve (AUC) (AUC) {cite}`Hanley1982-uo`).\n",
    "Another study may want its defect models to discover an actual defective module as early as possible (i.e., optimise based on initial false alarm {cite}`huang2017supervised`).\n",
    "Thus, several evaluation measures have been proposed to serve different goals of developing defect models.\n",
    "<!-- Examples are predictive accuracy measures (e.g., Area Under the ROC Curve (AUC) {cite}`Hanley1982-uo` and Matthews Correlation Coefficients (MCC) {cite}`matthews1975comparison`) and effort-aware measures (e.g., Distance-to-heaven {cite}`agrawal2018better` and Top k% LOC precision {cite}`huang2017supervised`). -->\n",
    "Below, we describe *5* predictive accuracy and *3* effort-aware measures in detail with interactive tutorials.\n",
    "We also provide a code snippet for setting up the environment for interactive tutorials below, i.e., data preparation and model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": [
     "remove-output",
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=10, random_state=1234)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Data and preparing datasets\n",
    "\n",
    "# Import for Load Data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import for Split Data into Training and Testing Samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import for Construct Defect Models (Classification)\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forests\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv((\"../../datasets/lucene-2.9.0.csv\"), index_col = 'File')\n",
    "test_dataset = pd.read_csv((\"../../datasets/lucene-3.0.0.csv\"), index_col = 'File')\n",
    "\n",
    "outcome = 'RealBug'\n",
    "features = ['OWN_COMMIT', 'Added_lines', 'CountClassCoupled', 'AvgLine', 'RatioCommentToCode']\n",
    "\n",
    "# process outcome to 0 and 1\n",
    "train_dataset[outcome] = pd.Categorical(train_dataset[outcome])\n",
    "train_dataset[outcome] = train_dataset[outcome].cat.codes\n",
    "\n",
    "test_dataset[outcome] = pd.Categorical(test_dataset[outcome])\n",
    "test_dataset[outcome] = test_dataset[outcome].cat.codes\n",
    "\n",
    "X_train = train_dataset.loc[:, features]\n",
    "X_test = test_dataset.loc[:, features]\n",
    "\n",
    "y_train = train_dataset.loc[:, outcome]\n",
    "y_test = test_dataset.loc[:, outcome]\n",
    "\n",
    "\n",
    "# commits - # of commits that modify the file of interest\n",
    "# Added lines - # of added lines of code\n",
    "# Count class coupled - # of classes that interact or couple with the class of interest\n",
    "# LOC - # of lines of code\n",
    "# RatioCommentToCode - The ratio of lines of comments to lines of code\n",
    "features = ['nCommit', 'AddedLOC', 'nCoupledClass', 'LOC', 'CommentToCodeRatio']\n",
    "\n",
    "X_train.columns = features\n",
    "X_test.columns = features\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "testing_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "## Construct defect models\n",
    "# Random Forests\n",
    "rf_model = RandomForestClassifier(random_state=1234, n_jobs = 10)\n",
    "rf_model.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Predictive Accuracy Measures\n",
    "\n",
    "\n",
    "#### Precision\n",
    "Precision measures the proportion between the number of lines that are correctly identified as defective and the number of lines that are identified by the models.\n",
    "Particularly, precision can be computed using a calculation of $\\frac{TP}{(TP+FP)}$, where $TP$ is the number of actual defective lines that are predicted as defective and $FP$ is the number of clean lines that are predicted as defective.\n",
    "A high precision value indicates that the models can correctly identify high number of defective lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct a confusion matrix:\n",
      "(True Positive, False Positive) = (99,147)\n",
      "(False Negative, True Negative) = (56,1035)\n",
      "\n",
      "Precision (manual calculation)\t\t: 0.4024390243902439\n",
      "Precision (precision_score function)\t: 0.4024390243902439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# construct a confusion matrix\n",
    "print('Construct a confusion matrix:')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,rf_model.predict(X_test)).ravel()\n",
    "print('(True Positive, False Positive) = (' + str(tp) +','+str(fp)+')')\n",
    "print('(False Negative, True Negative) = (' + str(fn) +','+str(tn)+')\\n')\n",
    "\n",
    "# calculate precision manually\n",
    "rf_precision_manual = tp/(tp + fp)\n",
    "\n",
    "# calculate precision with a function\n",
    "rf_precision_function = precision_score(y_test, rf_model.predict(X_test))\n",
    "\n",
    "print('Precision (manual calculation)\\t\\t:', rf_precision_manual)\n",
    "print('Precision (precision_score function)\\t:', rf_precision_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "Recall measures the proportion between the number of lines that are correctly identified as defective and the number of actual defective lines.\n",
    "More specifically, we compute recall using a calculation of $\\frac{TP}{(TP+FN)}$, where $TP$ is the number of actual defective lines that are predicted as defective and $FN$ is the number of actual defective lines that are predicted as clean.\n",
    "A high recall value indicates that the approach can identify more defective lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (manual calculation)\t\t: 0.6387096774193548\n",
      "Recall (recall_score function)\t: 0.6387096774193548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# calculate recall manually\n",
    "rf_recall_manual = tp/(tp + fn)\n",
    "\n",
    "# calculate recall with a function\n",
    "rf_recall_function = recall_score(y_test, rf_model.predict(X_test))\n",
    "\n",
    "print('Recall (manual calculation)\\t\\t:', rf_recall_manual)\n",
    "print('Recall (recall_score function)\\t:', rf_recall_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Alarm Rate (FAR) or False Positive Rate (FPR)\n",
    "FAR measures a proportion between the number of clean lines that are identified as defective and the number of actual clean lines.\n",
    "More specifically, FAR can be calculated with a calculation of $\\frac{FP}{(FP+TN)}$, where $FP$ is the number of actual clean lines that are predicted as defective and $TN$ is the number of actual clean lines that are predicted as clean.\n",
    "The lower the FAR value is, the fewer the clean lines that are identified as defective.\n",
    "In other words, a low FAR value indicates that developers spend less effort when inspecting defect-prone lines identified by the an approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAR (manual calculation)\t\t: 0.12436548223350254\n"
     ]
    }
   ],
   "source": [
    "# calculate FAR manually\n",
    "rf_FAR_manual = fp / (fp + tn)\n",
    "\n",
    "print('FAR (manual calculation)\\t\\t:', rf_FAR_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under the receiver operator characteristic Curve (AUC)\n",
    "AUC measures the discriminatory power of predictive models and is widely suggested by recent research {cite}`lessmann2008benchmarking`{cite}`ghotra2015revisiting`{cite}`rahman2013and`.\n",
    "The axes of the curve of the AUC measure are the coverage of non-defective modules (true negative rate) for the x-axis and the coverage of defective modules (true positive rate) for the y-axis.\n",
    "The AUC measure is a threshold-independent performance measure that evaluates the ability of models in discriminating between defective and clean instances.\n",
    "The values of AUC range between 0 (worst), 0.5 (no better than random guessing), and 1 (best) {cite}`Hanley1982-uo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (roc_auc_score function)\t\t: 0.8589023524916761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "# calculate AUC with a function\n",
    "rf_AUC_function = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:,1])\n",
    "\n",
    "print('AUC (roc_auc_score function)\\t\\t:', rf_AUC_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matthews Correlation Coefficients (MCC)\n",
    "MCC measures a correlation coefficients between actual and predicted outcomes using the following calculation:\n",
    "\\begin{equation}\n",
    "\\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "\\end{equation}\n",
    "An MCC value ranges from -1 to +1, where an MCC value of 1 indicates a perfect prediction, and -1 indicates total disagreement between the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC (manual calculation)\t\t: 0.4249604383453459\n",
      "MCC (matthews_corrcoef function)\t: 0.4249604383453459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# calculate MCC manually\n",
    "rf_MCC_manual = ((tp * tn) - (fp * fn)) / (((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** (1/2))\n",
    "\n",
    "# calculate MCC with a function\n",
    "rf_MCC_function = matthews_corrcoef(y_test, rf_model.predict(X_test))\n",
    "\n",
    "print('MCC (manual calculation)\\t\\t:', rf_MCC_manual)\n",
    "print('MCC (matthews_corrcoef function)\\t:', rf_MCC_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effort-Aware Measures\n",
    "\n",
    "#### Initial False Alarm (IFA)\n",
    "IFA measures the number of clean lines on which developers spend SQA effort until the first defective line is found when lines are ranked by their defect-proneness {cite}`huang2017supervised`.\n",
    "A low IFA value indicates that few clean lines are ranked at the top, while a high IFA value indicates that developers will spend unnecessary effort on clean lines.\n",
    "The intuition behinds this measure is that developers may stop inspecting if they could not get promising results (i.e., find defective lines) within the first few inspected lines {cite}`parnin2011automated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial False Alarm (IFA)\t\t: 1\n"
     ]
    }
   ],
   "source": [
    "# Generate a defect-proneness ranking of testing instances\n",
    "X_test_df = X_test.copy()\n",
    "X_test_df['predicted_prob'] = rf_model.predict_proba(X_test)[:, 1]\n",
    "X_test_df = X_test_df.sort_values(by = ['predicted_prob'], ascending = False)\n",
    "\n",
    "# Determine the Initial False Alarm (IFA)\n",
    "IFA = 0\n",
    "for test_index in X_test_df.index:\n",
    "    IFA += 1\n",
    "    if y_test.loc[test_index] == 1:\n",
    "        break\n",
    "        \n",
    "print('Initial False Alarm (IFA)\\t\\t:', IFA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance-to-Heaven (D2H)\n",
    "\n",
    "D2H is a combination of recall and FAR proposed by Agrawal and Menzies {cite}`agrawal2018better` {cite}`agrawal2019dodge`.\n",
    "The calculation of D2H is the root mean square of the recall and false alarm values (i.e., $\\sqrt{\\frac{(1-Recall)^2 + (0-FAR)^2}{2}}$).\n",
    "A d2h value of 0 indicates that an approach achieves a perfect identification, i.e., an approach can identify all defective lines (Recall $= 1$) without any false positives (FAR $= 0$).\n",
    "A high d2h value indicates that the performance of an approach is far from perfect, e.g., achieving a high recall value but also have high a FAR value and vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2H (manual calculation)\t\t: 0.27018278105904375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# calculate recall with a function\n",
    "rf_recall_function = recall_score(y_test, rf_model.predict(X_test))\n",
    "\n",
    "# calculate FAR manually\n",
    "rf_FAR_manual = fp / (fp + tn)\n",
    "\n",
    "# calculate D2H manually\n",
    "rf_D2H_numerator = ((1 - rf_recall_function)**2) + ((0 - rf_FAR_manual)**2)\n",
    "rf_D2H_denominator = 2\n",
    "rf_D2H_manual = (rf_D2H_numerator / rf_D2H_denominator)**(1/2)\n",
    "\n",
    "print('D2H (manual calculation)\\t\\t:', rf_D2H_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top k% LOC Precision\n",
    "Top k% LOC Precision measures how many defective lines found when inspecting the top k% of lines ranked by the defect-proneness estimated by the models {cite}`huang2017supervised`.\n",
    "A high value of Top k% LOC precision indicates that the model can rank many defective lines at the top and many defective lines can be found given the fixed amount of effort (i.e., k% of LOC).\n",
    "On the other hand, the low value of Top k% LOC precision indicates many clean lines are in the top k% LOC and developers need to inspect more lines to identify defects.\n",
    "Similar to prior studies {cite}`mende2010effort` {cite}`kamei2010revisiting` {cite}`rahman2014comparing` {cite}`ray2016naturalness`, we use 20% of LOC as a fixed cutoff for an effort in this interactive tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumsum_LOC = 3736 Index 231\n",
      "Top 20% LOC Precision (precision_score function)\t: 0.4025974025974026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Generate a defect-proneness ranking of testing instances\n",
    "X_test_df = X_test.copy()\n",
    "X_test_df['predicted_prob'] = rf_model.predict_proba(X_test)[:, 1]\n",
    "X_test_df = X_test_df.sort_values(by = ['predicted_prob'], ascending = False)\n",
    "\n",
    "# calculate the value of k% LOC, where k = 20\n",
    "k_percent = 20.0\n",
    "total_LOC = np.sum(X_test_df['LOC'])\n",
    "p20_LOC = total_LOC * k_percent / 100\n",
    "\n",
    "# find Top k% LOC according to the defect-proneness ranking of testing instances\n",
    "cumsum_LOC = 0\n",
    "last_index = -1\n",
    "for i in range(len(X_test)):\n",
    "    cumsum_LOC += X_test_df['LOC'].iloc[i]\n",
    "    last_index = i\n",
    "    \n",
    "    if cumsum_LOC > p20_LOC:\n",
    "        print('Cumsum_LOC =', cumsum_LOC, 'Index', last_index)\n",
    "        break\n",
    "p20_LOC_X_test_df = X_test_df.iloc[:last_index, :]\n",
    "p20_LOC_y_test_df = y_test[p20_LOC_X_test_df.index]\n",
    "\n",
    "# calculate precision of Top k% LOC\n",
    "rf_p20_precision_function = precision_score(p20_LOC_y_test_df, rf_model.predict(p20_LOC_X_test_df.loc[:, X_test.columns]))\n",
    "\n",
    "print('Top 20% LOC Precision (precision_score function)\\t:', rf_p20_precision_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top k% LOC Recall\n",
    "Top k%LOC recall measures how many actual defective lines found given a fixed amount of effort, i.e., the top k% of lines ranked by their defect-proneness {cite}`huang2017supervised`.\n",
    "A high value of top k% LOC recall indicates that an approach can rank many actual defective lines at the top and many actual defective lines can be found given a fixed amount of effort.\n",
    "On the other hand, a low value of top k% LOC recall indicates many clean lines are in the top k% LOC and developers need to spend more effort to identify defective lines.\n",
    "Similarly, we use 20% of LOC as a fixed cutoff for an effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumsum_LOC = 3736 Index 231\n",
      "Top 20% LOC Recall (recall_score function)\t: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Generate a defect-proneness ranking of testing instances\n",
    "X_test_df = X_test.copy()\n",
    "X_test_df['predicted_prob'] = rf_model.predict_proba(X_test)[:, 1]\n",
    "X_test_df = X_test_df.sort_values(by = ['predicted_prob'], ascending = False)\n",
    "\n",
    "# calculate the value of k% LOC, where k = 20\n",
    "k_percent = 20.0\n",
    "total_LOC = np.sum(X_test_df['LOC'])\n",
    "p20_LOC = total_LOC * k_percent / 100\n",
    "\n",
    "# find Top k% LOC according to the defect-proneness ranking of testing instances\n",
    "cumsum_LOC = 0\n",
    "last_index = -1\n",
    "for i in range(len(X_test)):\n",
    "    cumsum_LOC += X_test_df['LOC'].iloc[i]\n",
    "    last_index = i\n",
    "    \n",
    "    if cumsum_LOC > p20_LOC:\n",
    "        print('Cumsum_LOC =', cumsum_LOC, 'Index', last_index)\n",
    "        break\n",
    "p20_LOC_X_test_df = X_test_df.iloc[:last_index, :]\n",
    "p20_LOC_y_test_df = y_test[p20_LOC_X_test_df.index]\n",
    "\n",
    "# calculate recall of Top k% LOC\n",
    "rf_p20_recall_function = recall_score(p20_LOC_y_test_df, rf_model.predict(p20_LOC_X_test_df.loc[:, X_test.columns]))\n",
    "\n",
    "print('Top 20% LOC Recall (recall_score function)\\t:', rf_p20_recall_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Parts of this chapter have been published by Supatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Hideaki Hata, Kenichi Matsumoto: Predicting Defective Lines Using a Model-Agnostic Technique. CoRR abs/2009.03612 (2020).\n",
    "```\n",
    "\n",
    "## Suggested Readings\n",
    "\n",
    "\n",
    "[1] Amritanshu Agrawal, Tim Menzies: Is \"better data\" better than \"better data miners\"?: on the benefits of tuning SMOTE for defect prediction. ICSE 2018: 1050-1061.\n",
    "\n",
    "[2] Amritanshu Agrawal, Wei Fu, Di Chen, Xipeng Shen, Tim Menzies: How to \"DODGE\" Complex Software Analytics? CoRR abs/1902.01838 (2019).\n",
    "\n",
    "[3] Chris Parnin, Alessandro Orso: Are automated debugging techniques actually helping programmers? ISSTA 2011: 199-209.\n",
    "\n",
    "[4] Qiao Huang, Xin Xia, David Lo: Supervised vs Unsupervised Models: A Holistic Look at Effort-Aware Just-in-Time Defect Prediction. ICSME 2017: 159-170."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
