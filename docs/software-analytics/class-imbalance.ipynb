{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always handle class imbalance\n",
    "\n",
    "\n",
    "## Class Rebalancing Techniques for Software Analytics\n",
    "\n",
    "A plethora of class rebalancing techniques exist {cite}`he2009learning`,\n",
    "e.g., (1) sampling methods for imbalanced learning, (2) cost-sensitive\n",
    "methods for imbalanced learning, (3) kernel-based methods for imbalanced\n",
    "learning, and (4) active learning for imbalanced learning. Since it is\n",
    "impractical to study all of these techniques, we select a manageable set\n",
    "of class rebalancing techniques for our study. As discussed by\n",
    "He {cite}`he2009learning`, we start from the four families of imbalance\n",
    "learning techniques. Based on a literature surveys by Hall *et al.* {cite}`hall2012systematic`,\n",
    "Shihab {cite}`Shihab2012`, and Nam {cite}`nam2014survey`, we then select only the\n",
    "family of sampling techniques for the context of defect prediction.\n",
    "\n",
    "We first select the three commonly-used techniques (i.e., over-sampling,\n",
    "under-sampling, and Default SMOTE {cite}`chawla2002smote`) that were\n",
    "previously used in the literature {cite}`Kamei2007`{cite}`Khoshgoftaar2010`{cite}`Pelayo2007`{cite}`Seiffert2014`{cite}`tan2015online`{cite}`wang2013`{cite}`xia2015elblocker`{cite}`yang2016automated`{cite}`yang2017high`.\n",
    "Recent research shows that bootstrap resampling techniques tend to\n",
    "produce more accurate and reliable estimates in the context of software\n",
    "engineering {cite}`tantithamthavorn2016tse`. Recently,\n",
    "Menardi {cite}`menardi2014training` show that a smoothed bootstrap\n",
    "resampling technique (ROSE) outperforms other techniques in a\n",
    "non-software engineering domain. Thus, we select the ROSE\n",
    "technique {cite}`lunardon2014rose`{cite}`menardi2014training` in our study.\n",
    "\n",
    "Below, we provide a description and a discussion of the 4 studied class rebalancing techniques for our study with interactive python tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Load Data and preparing datasets\n",
    "\n",
    "# Import for Load Data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "# Import for Split Data into Training and Testing Samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Import for Construct a black-box model (Regression and Random Forests)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_dataset = pd.read_csv((\"../../datasets/lucene-2.9.0.csv\"), index_col = 'File')\n",
    "test_dataset = pd.read_csv((\"../../datasets/lucene-3.0.0.csv\"), index_col = 'File')\n",
    "\n",
    "outcome = 'RealBug'\n",
    "features = ['OWN_COMMIT', 'Added_lines', 'CountClassCoupled', 'AvgLine', 'RatioCommentToCode']\n",
    "\n",
    "# process outcome to 0 and 1\n",
    "train_dataset[outcome] = pd.Categorical(train_dataset[outcome])\n",
    "train_dataset[outcome] = train_dataset[outcome].cat.codes\n",
    "\n",
    "test_dataset[outcome] = pd.Categorical(test_dataset[outcome])\n",
    "test_dataset[outcome] = test_dataset[outcome].cat.codes\n",
    "\n",
    "X_train = train_dataset.loc[:, features]\n",
    "X_test = test_dataset.loc[:, features]\n",
    "\n",
    "y_train = train_dataset.loc[:, outcome]\n",
    "y_test = test_dataset.loc[:, outcome]\n",
    "\n",
    "\n",
    "# commits - # of commits that modify the file of interest\n",
    "# Added lines - # of added lines of code\n",
    "# Count class coupled - # of classes that interact or couple with the class of interest\n",
    "# LOC - # of lines of code\n",
    "# RatioCommentToCode - The ratio of lines of comments to lines of code\n",
    "features = ['nCommit', 'AddedLOC', 'nCoupledClass', 'LOC', 'CommentToCodeRatio']\n",
    "\n",
    "X_train.columns = features\n",
    "X_test.columns = features\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "testing_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Compute class ratio\n",
    "def get_class_ratio(input_data):\n",
    "    n_data = len(input_data)\n",
    "    n_defective = sum(input_data)\n",
    "    n_clean = n_data - n_defective\n",
    "    print('From the total of', n_data, 'files, there are:')\n",
    "    print(n_defective, 'defective files', '('+str(np.round(n_defective*1.0/n_data*100, 2))+'%)')\n",
    "    print(n_clean, 'clean files', '('+str(np.round(n_clean*1.0/n_data*100, 2))+'%)')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 1368 files, there are:\n",
      "273 defective files (19.96%)\n",
      "1095 clean files (80.04%)\n"
     ]
    }
   ],
   "source": [
    "# Find a class ratio of the original training dataset\n",
    "get_class_ratio(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-Sampling Technique (OVER)\n",
    "\n",
    "The over-sampling technique (a.k.a. up-sampling) randomly samples with\n",
    "replacement (i.e., replicating) *the minority class* (e.g., defective\n",
    "class) to be the same size as the majority class (e.g., clean class).\n",
    "The advantage of an over-sampling technique is that it leads to no\n",
    "information loss. Since oversampling simply adds replicated modules from\n",
    "the original dataset, the disadvantage is that the training dataset ends\n",
    "up with multiple redundant modules, leading to an overfitting. Thus,\n",
    "when applying the over-sampling technique, the performance of with-in\n",
    "defect prediction models is likely higher than the performance of\n",
    "cross-project defect prediction models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 2190 files, there are:\n",
      "1095 defective files (50.0%)\n",
      "1095 clean files (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Import for the Over-Sampling technique (OVER)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply the Over-Sampling technique\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_OVER_train, y_OVER_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Find a class ratio of the over-sampled training dataset\n",
    "get_class_ratio(y_OVER_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-Sampling Technique (UNDER)\n",
    "\n",
    "The under-sampling technique (a.k.a. down-sampling) randomly samples\n",
    "(i.e., reducing) *the majority class* (e.g., clean class) in order to\n",
    "reduce the number of majority modules to be the same number as the\n",
    "minority class (e.g., defective class). The advantage of an\n",
    "under-sampling technique is that it reduces the size of the training\n",
    "data when the original data is relatively large. However, the\n",
    "disadvantage is that removing modules may cause the training data to\n",
    "lose important information pertaining to the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 546 files, there are:\n",
      "273 defective files (50.0%)\n",
      "273 clean files (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Import for the Under-Sampling technique (UNDER)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Apply the Under-Sampling technique\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_UNDER_train, y_UNDER_train = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Find a class ratio of the under-sampled training dataset\n",
    "get_class_ratio(y_UNDER_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "The SMOTE technique {cite}`chawla2002smote` was proposed to combat the\n",
    "disavantages of the simple over-sampling and under-sampling techniques.\n",
    "The SMOTE technique creates artificial data based on the feature space\n",
    "(rather than the data space) similarities from the minority modules. The\n",
    "SMOTE technique starts with a set of minority modules (i.e., defective\n",
    "modules). For each of the minority defective modules of the training\n",
    "datasets, SMOTE performs the following steps:\n",
    "\n",
    "1.  Calculate the $k$-nearest neighbors.\n",
    "\n",
    "2.  Select $N$ majority clean modules based on the smallest magnitude of\n",
    "    the euclidean distances that are obtained from the $k$-nearest\n",
    "    neighbors.\n",
    "\n",
    "Finally, SMOTE combines the synthetic oversampling of the minority\n",
    "defective modules with the undersampling the majority clean modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the total of 2190 files, there are:\n",
      "1095 defective files (50.0%)\n",
      "1095 clean files (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Import for the Synthetic Minority Oversampling Technique (SMOTE)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply the SMOTE technique\n",
    "oversample_SMOTE = SMOTE(sampling_strategy='minority')\n",
    "X_SMOTE_train, y_SMOTE_train = oversample_SMOTE.fit_resample(X_train, y_train)\n",
    "\n",
    "# Find a class ratio of the SMOTE-ed training dataset\n",
    "get_class_ratio(y_SMOTE_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Boostrap Random Over-Sampling Examples Technique (ROSE)\n",
    "\n",
    "The ROSE technique {cite}`lunardon2014rose` uses a smoothed-bootstrapping\n",
    "approach to draw artificial samples from the feature space neighbourhood\n",
    "around the minority class {cite}`Efron1993`. ROSE combines oversampling and\n",
    "undersampling by generating an augmented sample of the data (especially\n",
    "belonging to the rare class). The ROSE technique consists of four steps:\n",
    "\n",
    "1.  Resample the data of the majority class using a bootstrap resampling\n",
    "    technique to remove modules of the majority class to a defective\n",
    "    ratio of 50% (undersampling).\n",
    "\n",
    "2.  Resample the data of the minority class using a bootstrap resampling\n",
    "    technique to repeat modules of the minority class to a defective\n",
    "    ratio of 50% (oversampling).\n",
    "\n",
    "3.  Combine the data of the majority and minority classes from Steps 1 and 2 into a new training sample.\n",
    "\n",
    "4.  Generate a new synthetic data for both the majority and minority\n",
    "    classes in its neighborhood {cite}`menardi2014training` based on the\n",
    "    combined data from Step 3. The shape of the neighborhood is\n",
    "    determined by the kernel density function with a Gaussian kernel $K$\n",
    "    and a smoothing matrix $\\textbf{H}$ with a $d$ dimension (i.e., $d$\n",
    "    is the number of independent variables), where $\\textbf{H}=diag(h_1,...,h_d)$ and $h_d$ is defined as follows:\n",
    "    \\\n",
    "    $$h_q = \\Bigg(\\frac{4}{(d+2)n}\\Bigg)^{1/(d+4)} \\times \\hat{\\sigma}_q ; q = 1,...,d.$$\n",
    "    , where $\\hat{\\sigma}_q$ is the standard deviation of the $q^{th}$\n",
    "    dimension of the observations belonging to a given\n",
    "    class {cite}`menardi2014training`.\n",
    "\n",
    "These four steps are repeated for each training sample in order to\n",
    "produce a new synthetic training sample of approximately equal size as\n",
    "the original dataset where the number of modules for both classes\n",
    "equally represent (i.e., a defective ratio of nearly 50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either TODO or Exclude\n",
    "# The python implementation is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
